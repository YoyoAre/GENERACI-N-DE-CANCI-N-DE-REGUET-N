{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1636858703975,
     "user": {
      "displayName": "2019 Mat FLORES MARTINEZ MONTSERRAT ILIANA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjRxdaFeq-wG9Q_cLz0LP59aI7CEeU491266zPs=s64",
      "userId": "15518882085329513130"
     },
     "user_tz": 360
    },
    "id": "Ms2dXckg-Kz7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A-IEUsYw-R4A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"Hector y Tito\\\n",
    "Luny Tunes y Noriega\\\n",
    "Con más\"\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9VJqAvJ1-TC4"
   },
   "outputs": [],
   "source": [
    "filename=\"pruebas.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wtTSOCId-Uwf"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(filename, encoding='ISO-8859-1') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "2rbMDxKQ-VFv"
   },
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "start_story = '|'*seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1635966452305,
     "user": {
      "displayName": "2019 Mat VELAZQUEZ MORAN PEDRO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11392187614295055784"
     },
     "user_tz": 360
    },
    "id": "Eo_68jRO-XtH",
    "outputId": "27adf6d5-967f-4a0c-ac4d-e622765d6d74"
   },
   "outputs": [],
   "source": [
    "text=text.lower()\n",
    "text=start_story+text\n",
    "#text=text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "#text=text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "text=text.replace('\\n', ' xoxo ')\n",
    "text=re.sub('  +', '. ', text).strip()\n",
    "text=text.replace('..', '.')\n",
    "\n",
    "text=re.sub('([!\"#$%&()*+,-./:;<=>¿?@[\\]^_`{|}~])\\'', r' \\1 ', text)\n",
    "text=re.sub('\\s{2,}', ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Wt9Ypzkx-ZRf"
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "tokenizer = Tokenizer(char_level=False, filters='')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) +1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pudin'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"pudin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1635966461758,
     "user": {
      "displayName": "2019 Mat VELAZQUEZ MORAN PEDRO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11392187614295055784"
     },
     "user_tz": 360
    },
    "id": "jN0nDavs-bAg",
    "outputId": "4b20320f-2883-495e-f069-666046ef431f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 104051 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_sequences(token_list, step):\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(0,len(token_list) - seq_length,step):\n",
    "    X.append(token_list[i:i + seq_length])\n",
    "    y.append(token_list[i + seq_length])\n",
    "  #esta funcion lo que hace es convertir un vector en una matriz tipo binaria\n",
    "  y = np_utils.to_categorical(y,num_classes = total_words)\n",
    "\n",
    "  num_seq = len(X)\n",
    "  print('Number of sequences:',num_seq, \"\\n\")\n",
    "\n",
    "  return X,y,num_seq\n",
    "\n",
    "step = 4\n",
    "seq_length = 20\n",
    "X,y,num_seq = generate_sequences(token_list,step)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27865965,
     "status": "ok",
     "timestamp": 1635994400848,
     "user": {
      "displayName": "2019 Mat VELAZQUEZ MORAN PEDRO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11392187614295055784"
     },
     "user_tz": 360
    },
    "id": "K4OZaKK_-c6w",
    "outputId": "0ab48733-d300-4cbd-ab52-d3050f02a595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3252/3252 [==============================] - 791s 238ms/step - loss: 5.9054\n",
      "Epoch 2/150\n",
      "3252/3252 [==============================] - 767s 236ms/step - loss: 5.4317\n",
      "Epoch 3/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 5.2719\n",
      "Epoch 4/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 5.2219\n",
      "Epoch 5/150\n",
      "3252/3252 [==============================] - 769s 236ms/step - loss: 5.4666\n",
      "Epoch 6/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 5.7888\n",
      "Epoch 7/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 5.9377\n",
      "Epoch 8/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 5.9764\n",
      "Epoch 9/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 5.9679\n",
      "Epoch 10/150\n",
      "3252/3252 [==============================] - 782s 240ms/step - loss: 5.9325\n",
      "Epoch 11/150\n",
      "3252/3252 [==============================] - 769s 236ms/step - loss: 5.8318\n",
      "Epoch 12/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 5.7494\n",
      "Epoch 13/150\n",
      "3252/3252 [==============================] - 786s 242ms/step - loss: 5.6345\n",
      "Epoch 14/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 5.5674\n",
      "Epoch 15/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 5.4821\n",
      "Epoch 16/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 5.3974\n",
      "Epoch 17/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 5.3239\n",
      "Epoch 18/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 5.2654\n",
      "Epoch 19/150\n",
      "3252/3252 [==============================] - 782s 240ms/step - loss: 5.2279\n",
      "Epoch 20/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 5.1925\n",
      "Epoch 21/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 5.1561\n",
      "Epoch 22/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 5.1038\n",
      "Epoch 23/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 5.0369\n",
      "Epoch 24/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 4.9897\n",
      "Epoch 25/150\n",
      "3252/3252 [==============================] - 769s 236ms/step - loss: 4.9635\n",
      "Epoch 26/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 4.9343\n",
      "Epoch 27/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.9097\n",
      "Epoch 28/150\n",
      "3252/3252 [==============================] - 772s 238ms/step - loss: 4.8621\n",
      "Epoch 29/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.8263\n",
      "Epoch 30/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 4.8099\n",
      "Epoch 31/150\n",
      "3252/3252 [==============================] - 779s 239ms/step - loss: 4.7819\n",
      "Epoch 32/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.7650\n",
      "Epoch 33/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.7507\n",
      "Epoch 34/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 4.7671\n",
      "Epoch 35/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 4.7517\n",
      "Epoch 36/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 4.7310\n",
      "Epoch 37/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 4.7100\n",
      "Epoch 38/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 4.7036\n",
      "Epoch 39/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.6749\n",
      "Epoch 40/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.6475\n",
      "Epoch 41/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 4.6219\n",
      "Epoch 42/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.5904\n",
      "Epoch 43/150\n",
      "3252/3252 [==============================] - 779s 240ms/step - loss: 4.5426\n",
      "Epoch 44/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 4.5065\n",
      "Epoch 45/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 4.4846\n",
      "Epoch 46/150\n",
      "3252/3252 [==============================] - 779s 240ms/step - loss: 4.4524\n",
      "Epoch 47/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 4.4464\n",
      "Epoch 48/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 4.4359\n",
      "Epoch 49/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 4.3860\n",
      "Epoch 50/150\n",
      "3252/3252 [==============================] - 782s 241ms/step - loss: 4.3532\n",
      "Epoch 51/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 4.3274\n",
      "Epoch 52/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 4.2915\n",
      "Epoch 53/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.2796\n",
      "Epoch 54/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 4.2464\n",
      "Epoch 55/150\n",
      "3252/3252 [==============================] - 784s 241ms/step - loss: 4.2399\n",
      "Epoch 56/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 4.2089\n",
      "Epoch 57/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 4.1955\n",
      "Epoch 58/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 4.1617\n",
      "Epoch 59/150\n",
      "3252/3252 [==============================] - 787s 242ms/step - loss: 4.1699\n",
      "Epoch 60/150\n",
      "3252/3252 [==============================] - 779s 240ms/step - loss: 4.1493\n",
      "Epoch 61/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.1722\n",
      "Epoch 62/150\n",
      "3252/3252 [==============================] - 774s 238ms/step - loss: 4.1691\n",
      "Epoch 63/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 4.1498\n",
      "Epoch 64/150\n",
      "3252/3252 [==============================] - 791s 243ms/step - loss: 4.1347\n",
      "Epoch 65/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 4.1468\n",
      "Epoch 66/150\n",
      "3252/3252 [==============================] - 787s 242ms/step - loss: 4.1138\n",
      "Epoch 67/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.1005\n",
      "Epoch 68/150\n",
      "3252/3252 [==============================] - 806s 248ms/step - loss: 4.0892\n",
      "Epoch 69/150\n",
      "3252/3252 [==============================] - 782s 240ms/step - loss: 4.0824\n",
      "Epoch 70/150\n",
      "3252/3252 [==============================] - 782s 241ms/step - loss: 4.0969\n",
      "Epoch 71/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 4.0831\n",
      "Epoch 72/150\n",
      "3252/3252 [==============================] - 804s 247ms/step - loss: 4.0943\n",
      "Epoch 73/150\n",
      "3252/3252 [==============================] - 797s 245ms/step - loss: 4.0706\n",
      "Epoch 74/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 4.0610\n",
      "Epoch 75/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 4.0483\n",
      "Epoch 76/150\n",
      "3252/3252 [==============================] - 784s 241ms/step - loss: 4.0329\n",
      "Epoch 77/150\n",
      "3252/3252 [==============================] - 786s 242ms/step - loss: 4.0240\n",
      "Epoch 78/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 4.0141\n",
      "Epoch 79/150\n",
      "3252/3252 [==============================] - 813s 250ms/step - loss: 3.9841\n",
      "Epoch 80/150\n",
      "3252/3252 [==============================] - 787s 242ms/step - loss: 3.9664\n",
      "Epoch 81/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.9490\n",
      "Epoch 82/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.9599\n",
      "Epoch 83/150\n",
      "3252/3252 [==============================] - 786s 242ms/step - loss: 3.9566\n",
      "Epoch 84/150\n",
      "3252/3252 [==============================] - 786s 242ms/step - loss: 3.9328\n",
      "Epoch 85/150\n",
      "3252/3252 [==============================] - 792s 243ms/step - loss: 3.9282\n",
      "Epoch 86/150\n",
      "3252/3252 [==============================] - 793s 244ms/step - loss: 3.9347\n",
      "Epoch 87/150\n",
      "3252/3252 [==============================] - 785s 241ms/step - loss: 3.9303\n",
      "Epoch 88/150\n",
      "3252/3252 [==============================] - 779s 239ms/step - loss: 3.9157\n",
      "Epoch 89/150\n",
      "3252/3252 [==============================] - 784s 241ms/step - loss: 3.9049\n",
      "Epoch 90/150\n",
      "3252/3252 [==============================] - 801s 246ms/step - loss: 3.9204\n",
      "Epoch 91/150\n",
      "3252/3252 [==============================] - 779s 240ms/step - loss: 3.9137\n",
      "Epoch 92/150\n",
      "3252/3252 [==============================] - 790s 243ms/step - loss: 3.8881\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3252/3252 [==============================] - 792s 243ms/step - loss: 3.9005\n",
      "Epoch 94/150\n",
      "3252/3252 [==============================] - 779s 240ms/step - loss: 3.8889\n",
      "Epoch 95/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 3.8903\n",
      "Epoch 96/150\n",
      "3252/3252 [==============================] - 797s 245ms/step - loss: 3.8867\n",
      "Epoch 97/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 3.8879\n",
      "Epoch 98/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 3.8670\n",
      "Epoch 99/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 3.8624\n",
      "Epoch 100/150\n",
      "3252/3252 [==============================] - 787s 242ms/step - loss: 3.8432\n",
      "Epoch 101/150\n",
      "3252/3252 [==============================] - 775s 238ms/step - loss: 3.8572\n",
      "Epoch 102/150\n",
      "3252/3252 [==============================] - 800s 246ms/step - loss: 3.8510\n",
      "Epoch 103/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.8423\n",
      "Epoch 104/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 3.8559\n",
      "Epoch 105/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.8692\n",
      "Epoch 106/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.8412\n",
      "Epoch 107/150\n",
      "3252/3252 [==============================] - 785s 241ms/step - loss: 3.8372\n",
      "Epoch 108/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 3.8235\n",
      "Epoch 109/150\n",
      "3252/3252 [==============================] - 848s 261ms/step - loss: 3.8093\n",
      "Epoch 110/150\n",
      "3252/3252 [==============================] - 781s 240ms/step - loss: 3.8135\n",
      "Epoch 111/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.8186\n",
      "Epoch 112/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 3.8112\n",
      "Epoch 113/150\n",
      "3252/3252 [==============================] - 770s 237ms/step - loss: 3.8177\n",
      "Epoch 114/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 3.8022\n",
      "Epoch 115/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 3.7968\n",
      "Epoch 116/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.7883\n",
      "Epoch 117/150\n",
      "3252/3252 [==============================] - 781s 240ms/step - loss: 3.8042\n",
      "Epoch 118/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.7977\n",
      "Epoch 119/150\n",
      "3252/3252 [==============================] - 773s 238ms/step - loss: 3.7933\n",
      "Epoch 120/150\n",
      "3252/3252 [==============================] - 786s 242ms/step - loss: 3.7855\n",
      "Epoch 121/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 3.8030\n",
      "Epoch 122/150\n",
      "3252/3252 [==============================] - 793s 244ms/step - loss: 3.7856\n",
      "Epoch 123/150\n",
      "3252/3252 [==============================] - 782s 240ms/step - loss: 3.7905\n",
      "Epoch 124/150\n",
      "3252/3252 [==============================] - 781s 240ms/step - loss: 3.7820\n",
      "Epoch 125/150\n",
      "3252/3252 [==============================] - 776s 239ms/step - loss: 3.7517\n",
      "Epoch 126/150\n",
      "3252/3252 [==============================] - 791s 243ms/step - loss: 3.7478\n",
      "Epoch 127/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 3.7399\n",
      "Epoch 128/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.7349\n",
      "Epoch 129/150\n",
      "3252/3252 [==============================] - 798s 245ms/step - loss: 3.7270\n",
      "Epoch 130/150\n",
      "3252/3252 [==============================] - 807s 248ms/step - loss: 3.7149\n",
      "Epoch 131/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 3.7165\n",
      "Epoch 132/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.7270\n",
      "Epoch 133/150\n",
      "3252/3252 [==============================] - 772s 237ms/step - loss: 3.7131\n",
      "Epoch 134/150\n",
      "3252/3252 [==============================] - 781s 240ms/step - loss: 3.7061\n",
      "Epoch 135/150\n",
      "3252/3252 [==============================] - 787s 242ms/step - loss: 3.6984\n",
      "Epoch 136/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 3.7202\n",
      "Epoch 137/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.7062\n",
      "Epoch 138/150\n",
      "3252/3252 [==============================] - 788s 242ms/step - loss: 3.7062\n",
      "Epoch 139/150\n",
      "3252/3252 [==============================] - 793s 244ms/step - loss: 3.7173\n",
      "Epoch 140/150\n",
      "3252/3252 [==============================] - 782s 240ms/step - loss: 3.6950\n",
      "Epoch 141/150\n",
      "3252/3252 [==============================] - 778s 239ms/step - loss: 3.7142\n",
      "Epoch 142/150\n",
      "3252/3252 [==============================] - 782s 241ms/step - loss: 3.7003\n",
      "Epoch 143/150\n",
      "3252/3252 [==============================] - 771s 237ms/step - loss: 3.6816\n",
      "Epoch 144/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.6894\n",
      "Epoch 145/150\n",
      "3252/3252 [==============================] - 777s 239ms/step - loss: 3.6971\n",
      "Epoch 146/150\n",
      "3252/3252 [==============================] - 834s 257ms/step - loss: 3.6750\n",
      "Epoch 147/150\n",
      "3252/3252 [==============================] - 792s 243ms/step - loss: 3.6759\n",
      "Epoch 148/150\n",
      "3252/3252 [==============================] - 791s 243ms/step - loss: 3.6801\n",
      "Epoch 149/150\n",
      "3252/3252 [==============================] - 781s 240ms/step - loss: 3.6871\n",
      "Epoch 150/150\n",
      "3252/3252 [==============================] - 780s 240ms/step - loss: 3.6797\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 100)         1920900   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 256)         365568    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19209)             4936713   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,748,493\n",
      "Trainable params: 7,748,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM, Input, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "n_units = 256\n",
    "embedding_size = 100\n",
    "\n",
    "text_in=Input(shape= (None,))\n",
    "#Embedding ocupa una dimensión de entrada que es el total de palabras que tenemos\n",
    "# y una de salida que en este caso es de 100. \n",
    "#Lo que hace esta función es convertir la entrada de vectores\n",
    "#binarios a un vector continuo denso\n",
    "#Estos vectores son una proyeccion de las palabras a un espacio continuo\n",
    "x=Embedding(total_words, embedding_size)(text_in)\n",
    "#El numero de unidades es el número de unidades paralelas  'identicas' pero\n",
    "#que cada una va aprender algo diferente.\n",
    "x = LSTM(n_units, return_sequences = True)(x)\n",
    "x = LSTM(n_units)(x)\n",
    "#Se pueden apilar redes LSTM para aprender mas caracteristicas\n",
    "x=Dropout(0.2)(x)\n",
    "text_out=Dense(total_words, activation='softmax')(x)\n",
    "\n",
    "model=Model(text_in, text_out)\n",
    "\n",
    "#opti=RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"RMSprop\")\n",
    "\n",
    "#Se usaron solo 10 epocas porque tardaba mucho en compilar con 100\n",
    "epochs=150\n",
    "batch_size=32\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "QWgE-vN0-ewH"
   },
   "outputs": [],
   "source": [
    "#La temperatura sirve para hacer más determinista el modelo. Una temperatura alta\n",
    "#hace al modelo más diverso en las respuestas, mas sensible a los ejemplos\n",
    "#pero tambien mas propenso a los errores\n",
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    #Se retorna el argumento con la probabilidad mas alta\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, temp):\n",
    "    output_text = seed_text\n",
    "    seed_text = start_story + seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        #print(token_list)\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "        \n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature = temp)\n",
    "        \n",
    "        output_word = tokenizer.index_word[y_class] if y_class>0 else ''\n",
    "        #Si se obtiene un \"|\" es que el modelo ya quiere terminar la historia\n",
    "        if output_word == \"|\":\n",
    "            break\n",
    "        seed_text += output_word + ' '\n",
    "        output_text += output_word + ' '          \n",
    "            \n",
    "    return output_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "error",
     "timestamp": 1636041192489,
     "user": {
      "displayName": "2019 Mat VELAZQUEZ MORAN PEDRO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11392187614295055784"
     },
     "user_tz": 360
    },
    "id": "1xw4-76c-hbm",
    "outputId": "29e5c46f-dbbc-49db-d386-fd7002549d56"
   },
   "outputs": [],
   "source": [
    "#La semilla tiene que se run texto con 20 palabras que es la longitud de secuencia\n",
    "seed_text = \"Bebe eres increible en la cama y yo te quiero comer tu cuerpo es todo lo que necesito para seguir contigo en la playa\"\n",
    "gen_words = 500\n",
    "temp = 0.1 \n",
    "cancion = generate_text(seed_text, gen_words, model, seq_length, temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "error",
     "timestamp": 1636041189493,
     "user": {
      "displayName": "2019 Mat VELAZQUEZ MORAN PEDRO",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11392187614295055784"
     },
     "user_tz": 360
    },
    "id": "qvqRPu8C-hWP",
    "outputId": "7d710a95-f84c-4d34-feb7-adf456b14ba2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bebe eres increible en la cama y yo te quiero comer tu cuerpo es todo lo que necesito para seguir contigo en la playa \\n  calle no es ( oh )  \\n  lo que tú digas que no piso sudor , les saben , yeah - real ( un par de un destino )  \\n   \\n  si se siente la noche ( luny - mientras los quieras )  \\n  cuando yo me ( de ti )  \\n  quiero que me more mirando  \\n  les vas hasta que se pierde ando ( wuh )  \\n   \\n  yo tengo una ma bien que yo te todos suena ( yeah )  \\n  que tú quieres de mí , yo por eso  \\n  por favor  \\n  ( jajajaja )  \\n  qué sabes que yo no hay miedo  \\n  con el que me dé sudando , que tu cuerpo se dé \"  \\n  no se pa\\' hacer y tú y mi vida  \\n  esta para que nos de la vida ?  \\n  tu ? tú es una duro yo lo que yo  \\n  muy bien  \\n   \\n  que me da ( ¡duro ! ) , por todo tú te amo y  \\n  y déjame tenerte en el barrio ( tú te juro )  \\n  cuando yo te ( oh - oh )  \\n  yo te quiero ver ( oh - oh , oh )  \\n  y lo he que estamos me me  \\n   \\n  sin par de que vengo  \\n  pa\\' que cuando se da  \\n  y de nuevo las noche un par de í  \\n  los ? si los que te hmm de  \\n  la y un destino que el urlcopyembedcopy  \\n  .  \\n  . '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancion=cancion.replace('xoxo', ' \\n ')\n",
    "cancion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = open(\"Cancion2.txt\",\"w\")\n",
    "nombres.write(cancion)\n",
    "nombres.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lsrm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "proyecto_lstm_word.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
